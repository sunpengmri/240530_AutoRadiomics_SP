{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will analyze Desmoid dataset from the [WORC Database](https://github.com/MStarmans91/WORCDatabase/tree/development).\n",
    "\n",
    "The task is to correctly identify segmented lesions as either **desmoid-type fibromatosis** or **extremity soft-tissue sarcoma**.\n",
    "\n",
    "\n",
    "More details on the dataset as well as the original analysis performed by their authors can be found here:\n",
    "\n",
    "`Starmans, M. P. A. et al. (2021). The WORC* database: MRI and CT scans, segmentations, and clinical labels for 932 patients from six radiomics studies. Submitted, preprint available from https://doi.org/10.1101/2021.08.19.21262238`\n",
    "\n",
    "`The experiments are described in the following paper: Starmans, M. P. A. et al. (2021). Reproducible radiomics through automated machine learning validated on twelve clinical applications. Submitted, preprint available from https://arxiv.org/abs/2108.08618.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from autorad.external.download_WORC import download_WORCDatabase\n",
    "\n",
    "# Set where we will save our data and results\n",
    "project_path=\"/home/peng/01_data/02_radiomics/demo\"\n",
    "base_dir = Path(project_path)\n",
    "# data_dir = base_dir / \"data\"\n",
    "# result_dir = base_dir / \"results\"\n",
    "data_dir = base_dir / \"data\"\n",
    "result_dir = base_dir / \"results\"\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "result_dir.mkdir(exist_ok=True, parents=True)\n",
    "os.environ[\"AUTORAD_RESULT_DIR\"] = str(result_dir)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.utils.preprocessing import get_paths_with_separate_folder_per_case\n",
    "\n",
    "# create a table with all the paths\n",
    "paths_df = get_paths_with_separate_folder_per_case(data_dir, relative=True)\n",
    "paths_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.data import ImageDataset\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "image_dataset = ImageDataset(\n",
    "    paths_df,\n",
    "    ID_colname=\"ID\",\n",
    "    root_dir=data_dir,\n",
    ")\n",
    "\n",
    "# Let's take a look at the data, plotting random 10 cases\n",
    "image_dataset.plot_examples(n=10, window=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tumor delineation check\n",
    "## 2.1 using Dice similarity to calc mask reproducibility, should set a propper cut-off value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Dice similarity\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import dice\n",
    "\n",
    "def calculate_dice_scipy(mask1_path, mask2_path):\n",
    "    # Load the masks\n",
    "    mask1 = nib.load(mask1_path).get_fdata()\n",
    "    mask2 = nib.load(mask2_path).get_fdata()\n",
    "\n",
    "    # Binarize the masks\n",
    "    mask1 = np.where(mask1 > 0, 1, 0)\n",
    "    mask2 = np.where(mask2 > 0, 1, 0)\n",
    "\n",
    "    # Flatten the masks\n",
    "    mask1 = mask1.flatten()\n",
    "    mask2 = mask2.flatten()\n",
    "\n",
    "    # Calculate Dice dissimilarity\n",
    "    dice_dissimilarity = dice(mask1, mask2)\n",
    "\n",
    "    # Calculate Dice similarity\n",
    "    dice_similarity = 1. - dice_dissimilarity\n",
    "\n",
    "    return dice_similarity\n",
    "\n",
    "# Usage:\n",
    "# dice = calculate_dice_scipy('path_to_mask1.nii.gz', 'path_to_mask2.nii.gz')\n",
    "# print(dice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.feature_extraction import FeatureExtractor\n",
    "\n",
    "extractor = FeatureExtractor(image_dataset, extraction_params=\"MR_default.yaml\", n_jobs=-1)\n",
    "feature_df = extractor.run()\n",
    "\n",
    "cols_to_drop = feature_df.columns[[1, 2, 3]]  # Indexing is 0-based, so 1, 2, 3 represent the 2nd, 3rd, and 4th columns\n",
    "feature_df = feature_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(data_dir / \"labels.csv\")\n",
    "label_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check unique IDs in both dataframes\n",
    "\n",
    "unique_feature_ids = feature_df['ID'].unique()\n",
    "unique_label_ids = label_df['patient_ID'].unique()\n",
    "\n",
    "# Find IDs in feature_df that are not in label_df\n",
    "missing_ids = set(unique_feature_ids) - set(unique_label_ids)\n",
    "\n",
    "print(f\"Missing IDs: {missing_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "merged_feature_df = feature_df.merge(label_df, left_on=\"ID\",\n",
    "    right_on=\"patient_ID\", how=\"left\")\n",
    "\n",
    "# merged_feature_df = feature_df.merge(label_df, left_on=\"ID\",\n",
    "#     right_on=\"patient_ID\", how=\"right\")\n",
    "merged_feature_df.to_csv(data_dir / \"features.csv\", index=False)\n",
    "merged_feature_df = pd.read_csv(data_dir / \"features.csv\") # Read back the saved file to make sure it was saved correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# simulate some random data to demonstrate the feature icc calculation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get the first column name\n",
    "first_column = merged_feature_df.columns[0]\n",
    "# Get the last two column names\n",
    "last_two_columns = merged_feature_df.columns[-2:]\n",
    "# Drop the first and last two columns\n",
    "merged_feature_df_tmp = merged_feature_df.drop(columns=[first_column, *last_two_columns])\n",
    "std_dev = merged_feature_df_tmp.std()\n",
    "\n",
    "\n",
    "merged_feature_df.shape\n",
    "random_df = pd.DataFrame(np.random.rand(*merged_feature_df_tmp.shape), columns=merged_feature_df_tmp.columns)* std_dev\n",
    "\n",
    "merged_feature_df_2 = merged_feature_df_tmp + random_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. feature preprocess\n",
    "## 4.1 intraclass correlation coefficients (ICCs) from different masks delineated by different doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "\n",
    "# Ensure both dataframes have the same columns in the same order\n",
    "assert set(merged_feature_df_tmp.columns) == set(merged_feature_df_2.columns)\n",
    "merged_feature_df_tmp = merged_feature_df_tmp[merged_feature_df_2.columns]\n",
    "\n",
    "# Initialize an empty dictionary to store the ICC values\n",
    "icc_values = {}\n",
    "# Loop over the columns\n",
    "for column in merged_feature_df_tmp.columns:\n",
    "    # Create a new dataframe that combines the data from the same column in both dataframes\n",
    "    df_tmp = merged_feature_df_tmp[[column]].copy()\n",
    "    df_tmp['rater'] = 'rater1'\n",
    "    df_tmp['subject'] = df_tmp.index\n",
    "    df_2 = merged_feature_df_2[[column]].copy()\n",
    "    df_2['rater'] = 'rater2'\n",
    "    df_2['subject'] = df_2.index\n",
    "\n",
    "    df = pd.concat([df_tmp, df_2])\n",
    "    # df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    # # Calculate the ICC and store the result in the dictionary\n",
    "    icc = pg.intraclass_corr(data=df, targets='subject', raters='rater', ratings=column,nan_policy=\"omit\")['ICC'].values[2] # ICC ICC(3,1)\n",
    "    icc_values[column] = icc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "icc_df = pd.DataFrame(list(icc_values.items()), columns=['Features', 'ICC'])\n",
    "icc_selected = icc_df[icc_df['ICC'] > 0.965]\n",
    "print(icc_selected.index)\n",
    "print(icc_selected['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "feature_icc_selected = merged_feature_df[icc_selected['Features']]\n",
    "\n",
    "feature_icc_selected.to_csv(data_dir / \"feature_icc_selected.csv\", index=False)\n",
    "# feature_icc_selected['ID'] = merged_feature_df['ID']\n",
    "# feature_icc_selected['diagnosis'] = merged_feature_df['diagnosis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 variance of each feature value \n",
    "low variance (<75%) will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# calculate variance of each feature\n",
    "# low variance (<75%) will be removed\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Initialize a VarianceThreshold feature selector\n",
    "selector = VarianceThreshold(threshold=0.1)  # Adjust the threshold as needed\n",
    "\n",
    "# Fit the selector to the data and transform the data\n",
    "feature_icc_var_selected = selector.fit_transform(feature_icc_selected)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "feature_icc_var_selected = pd.DataFrame(feature_icc_var_selected, columns=feature_icc_selected.columns[selector.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 chi2 of each feature value \n",
    "best 10 feature based chi2 was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# Find the minimum value in the DataFrame\n",
    "min_value = feature_icc_var_selected.min().min()\n",
    "# If the minimum value is less than 0, shift all values by its absolute value\n",
    "if min_value < 0:\n",
    "    feature_icc_var_selected += abs(min_value)\n",
    "\n",
    "selector = SelectKBest(chi2, k=10)\n",
    "\n",
    "feature_icc_var_chi2_selected = selector.fit_transform(feature_icc_var_selected, merged_feature_df['diagnosis'])\n",
    "feature_icc_var_chi2_selected.shape\n",
    "feature_icc_var_chi2_selected = pd.DataFrame(feature_icc_var_chi2_selected, columns=feature_icc_var_selected.columns[selector.get_support()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(feature_icc_var_chi2_selected.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 mutual information of each feature value \n",
    "best 10 feature based  mutual information was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Initialize a SelectKBest feature selector\n",
    "selector = SelectKBest(mutual_info_classif, k=10)  # Adjust the number of features as needed\n",
    "\n",
    "# Fit the selector to the data and transform the data\n",
    "feature_selected = selector.fit_transform(feature_icc_var_selected, merged_feature_df['diagnosis'])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "feature_icc_var_mi_selected = pd.DataFrame(feature_selected, columns=feature_icc_var_selected.columns[selector.get_support()])\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = feature_icc_var_selected.columns[selector.get_support()]\n",
    "\n",
    "print('Selected features:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 variance inflation factor (VIF)\n",
    "\n",
    "If you want to select features that are least correlated with each other, you can use the concept of variance inflation factor (VIF). VIF measures the correlation and multicollinearity of features. A high VIF value for a feature means that feature is highly correlated with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# calculate Pearson correlation between features\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(feature_icc_var_selected.values, i) for i in range(feature_icc_var_selected.shape[1])]\n",
    "vif[\"features\"] = feature_icc_var_selected.columns\n",
    "\n",
    "# Sort by VIF Factor in ascending order\n",
    "vif = vif.sort_values(\"VIF Factor\")\n",
    "\n",
    "# Select the features with the lowest VIF\n",
    "selected_features = vif[\"features\"].head(10)  # Adjust the number of features as needed\n",
    "\n",
    "print('Selected features:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Pearson correlation coefficient\n",
    "# large correlation coefficient (>0.75) will be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 t-test or Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# t-test between groups or Mann-Whitney U test\n",
    "from scipy.stats import mannwhitneyu,ttest_ind,shapiro\n",
    "\n",
    "# Define an empty list to hold the selected features\n",
    "selected_features = []\n",
    "\n",
    "# Perform the Mann-Whitney U test for each feature\n",
    "for feature in feature_icc_var_selected.columns:\n",
    "    group1 = feature_icc_var_selected.loc[merged_feature_df['diagnosis'] == 0, feature]\n",
    "    group2 = feature_icc_var_selected.loc[merged_feature_df['diagnosis'] == 1, feature]\n",
    "    \n",
    "    # Test the normality of the feature in each group\n",
    "    _, p1 = shapiro(group1)\n",
    "    _, p2 = shapiro(group2)\n",
    "    \n",
    "    # If both groups are normally distributed, perform the t-test\n",
    "    if p1 > 0.05 and p2 > 0.05:\n",
    "        stat, p = ttest_ind(group1, group2)\n",
    "    # Otherwise, perform the Mann-Whitney U test\n",
    "    else:\n",
    "        stat, p = mannwhitneyu(group1, group2)\n",
    "    \n",
    "    # If the p-value is less than 0.05, add the feature to the list of selected features\n",
    "    if p < 0.05:\n",
    "        selected_features.append(feature)\n",
    "\n",
    "print('Selected features:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. generate feature_dataset for model trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 generate feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.data import FeatureDataset\n",
    "# merged_feature_df=pd.concat([merged_feature_df.iloc[:100, :300], merged_feature_df.iloc[:100, -2:]], axis=1)\n",
    "feature_dataset = FeatureDataset( \n",
    "    merged_feature_df,\n",
    "    target=\"diagnosis\",\n",
    "    ID_colname=\"ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Split the data into training/validation/test sets with stratification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "splits_path = result_dir / \"splits.yaml\"\n",
    "feature_dataset.split(method=\"train_with_cross_validation_test\", save_path=splits_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.models import MLClassifier\n",
    "\n",
    "models = MLClassifier.initialize_default_sklearn_models()\n",
    "\n",
    "models = [models[1]]\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.preprocessing import run_auto_preprocessing\n",
    "\n",
    "run_auto_preprocessing(\n",
    "    data=feature_dataset.data,\n",
    "    use_feature_selection=True,\n",
    "    # feature_selection_methods=[\"anova\", \"lasso\", \"boruta\"], # add None for optional no feature selection\n",
    "    feature_selection_methods=[\"anova\"],\n",
    "    use_oversampling=None,\n",
    "    result_dir=result_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.preprocessing import run_auto_preprocessing\n",
    "\n",
    "run_auto_preprocessing(\n",
    "    data=feature_dataset.data,\n",
    "    use_feature_selection=True,\n",
    "    # feature_selection_methods=[\"anova\", \"lasso\", \"boruta\"], # add None for optional no feature selection\n",
    "    feature_selection_methods=[\"boruta\"],\n",
    "    use_oversampling=False,\n",
    "    result_dir=result_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.training import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    dataset=feature_dataset,\n",
    "    models=models,\n",
    "    result_dir=result_dir,\n",
    ")\n",
    "\n",
    "experiment_name = \"WORC_tutorial\"\n",
    "\n",
    "trainer.set_optimizer(\"optuna\", n_trials=200)\n",
    "trainer.run(auto_preprocess=True, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.utils import mlflow_utils\n",
    "\n",
    "mlflow_utils.start_mlflow_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to inspect the training results in MLFlow dashboard at the shown address (http://localhost:8000 unless already taken). Make sure WORC_tutorial is selected as the experiment in the dashboard. \n",
    "\n",
    "The dashboard should look like the one below. You can click on a specific run to show more details and its artifacts.\n",
    "You can also rerun preprocessing and training with different parameters, which should create a new run in MLFlow. Finally, the best existing run will be selected for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='mlflow.png') # Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate the model on the unseen test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.visualization import plotly_utils\n",
    "from autorad.inference import infer_utils\n",
    "from autorad import evaluation\n",
    "\n",
    "artifacts = infer_utils.get_artifacts_from_best_run(experiment_name)\n",
    "\n",
    "result_df = evaluation.evaluate_feature_dataset(\n",
    "    dataset=feature_dataset,\n",
    "    model=artifacts[\"model\"],\n",
    "    preprocessor=artifacts[\"preprocessor\"],\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "plotly_utils.plot_roc_curve(result_df.y_true, result_df.y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall plot below can show us how many cases were classified:\n",
    "- correctly as fibromatosis (negative) -> green bars facing downward\n",
    "- falsely as fibromatosis -> red bars facing downward\n",
    "- correctly as sarcoma -> red bars facing upward\n",
    "- falsely as fibromatosis -> green bars facing upward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "plotly_utils.plot_waterfall(result_df.y_true, result_df.y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the model managed to overfit the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "train_result_df = evaluation.evaluate_feature_dataset(\n",
    "    dataset=feature_dataset,\n",
    "    model=artifacts[\"model\"],\n",
    "    preprocessor=artifacts[\"preprocessor\"],\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "plotly_utils.plot_roc_curve(train_result_df.y_true, train_result_df.y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, for reference, validation set used in hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "val_result_df = evaluation.evaluate_feature_dataset(\n",
    "    dataset=feature_dataset,\n",
    "    model=artifacts[\"model\"],\n",
    "    preprocessor=artifacts[\"preprocessor\"],\n",
    "    split=\"val\"\n",
    ")\n",
    "\n",
    "plotly_utils.plot_roc_curve(val_result_df.y_true, val_result_df.y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simple inference from image-mask pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.inference import infer\n",
    "\n",
    "img_path = data_dir / \"Desmoid-046\" / \"image.nii.gz\"\n",
    "seg_path = data_dir / \"Desmoid-046\" / \"segmentation.nii.gz\"\n",
    "\n",
    "\n",
    "inferrer = infer.Inferrer(\n",
    "    extraction_config=artifacts[\"extraction_config\"],\n",
    "    model=artifacts[\"model\"],\n",
    "    preprocessor=artifacts[\"preprocessor\"],\n",
    "    result_dir=result_dir,\n",
    ")\n",
    "\n",
    "# Get class probability only\n",
    "pred = inferrer.predict_proba(img_path, seg_path)\n",
    "print(f\"Probability of the case being sarcoma is: {pred:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. More comprehensive inference from image-mask pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from autorad.inference import infer_utils\n",
    "\n",
    "\n",
    "feature_df, X_preprocessed, pred = inferrer.predict_proba_with_features(img_path, seg_path)\n",
    "\n",
    "print(f\"Prediction: {pred:.3f}\")\n",
    "print(\"Features:\")\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "infer_utils.plot_shap_waterfall(artifacts[\"explainer\"], X_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHAP plot above shows which features contributed most to the predicted probability of the lesion being sarcoma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autorad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
